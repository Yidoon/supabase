import Layout from '~/layouts/DefaultGuideLayout'

export const meta = {
  id: 'storage-schema-optimisations',
  title: 'Storage Optimisations',
  description: 'Learn how to optimize Storage',
  subtitle: 'Learn how to optimize Storage',
  sidebar_label: 'Schema',
}

When your Storage project grows and hitting the Millions for stored assets, there are a few optimisations that we encourage to look for.

## Egress

If you are experiencing a high amount of egress, there are optimisations that will help you reduce it drastically.

#### Resize Images
Images usually contributes to the majority of your Egress bill, by keeping their total size as small as possible you'll get the advantage to reduce the bytes you are transferring, therefore reducing your Egress.
You can use our [Image Transformation](/guides/storage/serving/image-transformations) to optimise any image on fly.


#### Browser Cache
Utilising the browser cache will help reducing your egress as the asset will be stored on the user local cache after the first download.
When possible set a high `cache-control` value when uploading assets, this value will be the amount of time the asset will be stored on your users local cache.

#### Limit the upload size
Limiting the maximum file size that a user can upload, will help you keep the egress cost under control.
If you don't have any limits, users might upload large images,documents and various files.
You can limit the max file size by setting this option at the []bucket level](/guides/storage/buckets/creating-buckets)

## Optimize Listing Objects
When you reach a high number of records you might be noticing that `supabase.storage.list()` method starts getting slow.

This is because the endpoint is very generic and tries to compute folders and objects in a single query.
It is very useful when building something like the Storage viewer on the Supabase Admin Studio.

However, if your application don't need the entire hierarchy computed you can speed up drastically the query execution for listing your objects.
You can create a custom `rpc` function as following:

```sql
create or replace function list_objects(
    bucketid text,
    prefix text,
    limits int default 100,
    offsets int default 0
) returns table (
    name text,
    id uuid,
    updated_at timestamptz,
    created_at timestamptz,
    last_accessed_at timestamptz,
    metadata jsonb
) as $$
begin
    return query SELECT
        objects.name,
        objects.id,
        objects.updated_at,
        objects.created_at,
        objects.last_accessed_at,
        objects.metadata
    FROM storage.objects
    WHERE objects.name like prefix || '%'
    AND bucket_id = bucketid
    ORDER BY name ASC
    LIMIT limits
    OFFSET offsets;
end;
$$ language plpgsql stable;
```

You can then use the your custom rpc function as following:

Using SQL:
```sql
SELECT * from list_objects('bucket_id', '', 100, 0);
```

Using the SDK:

```js
const { data, error } = await supabase.rpc('list_objects', {
 bucketid: 'yourbucket',
 prefix: '',
 limit: 100,
 offset: 0
})
```

## Optimising RLS
When creating RLS policies against the storage tables you can add indexes to the interested columns to speed up the lookup

export const Page = ({ children }) => <Layout meta={meta} children={children} />

export default Page